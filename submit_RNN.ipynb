{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2da36a4-5a69-40a6-b4bc-f0379fe0ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5eb859-6349-40ba-8d99-949f7c7a3c11",
   "metadata": {},
   "source": [
    "# 【問題1】SimpleRNNのフォワードプロパゲーション実装\n",
    "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。\n",
    "\n",
    "\n",
    "バッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。\n",
    "\n",
    "$$a_t = x_{t}\\cdot W_{x} + h_{t-1}\\cdot W_{h} + B\\\\\n",
    "h_t = tanh(a_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa2192b0-5f05-4ac4-aa4e-8d9172691b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"  \n",
    "        \n",
    "        batch_size = x.shape[0] # 1\n",
    "        n_sequences = x.shape[1] # 3\n",
    "        n_features = x.shape[2] # 2\n",
    "        n_nodes = w_x.shape[1] # 4\n",
    "        \n",
    "        WX = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "        Wh = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "        \n",
    "        h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "        B = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
    "        \n",
    "        # batchサイズごとに\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            # sequence回繰り返す\n",
    "            for s in range(n_sequences):\n",
    "                \n",
    "                # forward\n",
    "                AT = X[i, s].reshape(batch_size, n_features) @ WX + h @ Wh + B\n",
    "                \n",
    "                # activation層\n",
    "                h = np.tanh(AT)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2834063-744d-46b8-bf04-1013b4cee874",
   "metadata": {},
   "source": [
    "# 【問題2】小さな配列でのフォワードプロパゲーションの実験\n",
    "小さな配列でフォワードプロパゲーションを考えてみます。\n",
    "\n",
    "\n",
    "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
    "\n",
    "\n",
    "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9fabc13-1f1b-446a-99e1-c011427a8777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b5ed07a-3830-426e-985b-c3d68546743c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = SimpleRNN()\n",
    "test.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb3043-42ee-49f5-aa92-464b84101a5b",
   "metadata": {},
   "source": [
    "# 【問題3】（アドバンス課題）バックプロパゲーションの実装\n",
    "バックプロパゲーションを実装してください。\n",
    "\n",
    "\n",
    "RNNの内部は全結合層を組み合わせた形になっているので、更新式は全結合層などと同様です。\n",
    "\n",
    "$$W_x^{\\prime} = W_x - \\alpha \\frac{\\partial L}{\\partial W_x} \\\\\n",
    "W_h^{\\prime} = W_h - \\alpha \\frac{\\partial L}{\\partial W_h} \\\\\n",
    "B^{\\prime} = B - \\alpha \\frac{\\partial L}{\\partial B}$$\n",
    "\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_x}$ : $W_x$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_h}$ : $W_h$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B}$ : $B$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "勾配を求めるためのバックプロパゲーションの数式が以下です。\n",
    "\n",
    "\n",
    "$\\frac{\\partial h_t}{\\partial a_t} = \\frac{\\partial L}{\\partial h_t} ×(1-tanh^2(a_t))$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B} = \\frac{\\partial h_t}{\\partial a_t}$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_x} = x_{t}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_h} = h_{t-1}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}$\n",
    "\n",
    "\n",
    "＊$\\frac{\\partial L}{\\partial h_t}$ は前の時刻からの状態の誤差と出力の誤差の合計です。hは順伝播時に出力と次の層に伝わる状態双方に使われているからです。\n",
    "\n",
    "\n",
    "前の時刻や層に流す誤差の数式は以下です。\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial h_{t-1}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{h}^{T}$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_{t}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{x}^{T}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bbed3e-7eca-49b9-97f5-ec958f3b8f0c",
   "metadata": {},
   "source": [
    "# 考察\n",
    "\n",
    "RNNは再帰的NNと呼ばれる。  \n",
    "上の実装でもわかる通り、入力データに時間軸を設けてその順番通りに重みを得ていく。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5412cc31-8799-43b7-a1fb-46b7c3e98443",
   "metadata": {},
   "source": [
    "# 疑問\n",
    "理屈は単純で理解ができた。  \n",
    "帰ってくる誤差では時系列のデータは持っていないが、どうやってhの誤差を更新するのだろうか。  \n",
    "計算式上hを求めた後、それを再度　dAで微分してXとh-1の重みを得てる？？  \n",
    "そのあと再度h-1で更新された重みを使用してd(h-1), dXの値を求める。   \n",
    "そのあとまたh-2を使用して....と繰り返してw(0)まで行うことで最終的な重みを得ているっぽい。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
